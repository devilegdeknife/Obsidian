---
title: 计算机原理哪些核心理念至今未变？
url: https://www.zhihu.com/question/1964648824472573270/answer/2005005249941889309
author: 编程大帅
author_badge: 一个程序员而已 求求关注吧
created: 2026-02-11 19:48
modified: 2026-02-11 19:48
upvote_num: 576
comment_num: 28
---
先说一个可能有点反直觉的判断：**你列举的那些东西——冯诺依曼架构、图灵机、布尔代数——它们的”不变程度”是完全不同的，不能放在一起讨论。** 有的确实是永恒的，有的早就被改得面目全非了，只是教科书更新太慢让你以为它还是原来的样子。

我做开发七八年，日常写Java和Go，跟底层打交道的机会不算少。之前做边缘计算项目要跟硬件团队对接，也帮公司排查过不少涉及底层的性能问题。在这个过程中有一个感受越来越强烈：真正在关键时刻指导我理解问题的，从来不是某个具体的架构设计长什么样，而是那些更底层的、你绕不过去的”约束”。具体的架构只是人类在约束下做的某个选择，选择会变，约束不会。我试着把这个感受讲清楚。


---


先说冯诺依曼架构，因为这是大家最常拿来举例的”不变原理”。但说实话，这东西严格来讲变了很多。你把1945年冯诺依曼写的那份EDVAC报告和现在你电脑里的CPU放在一起看，几乎认不出它们是同一回事。冯诺依曼架构的核心思想就三条：存储程序、顺序执行、统一的内存存放指令和数据。现在的CPU在干什么呢？乱序执行——指令不按你写的顺序跑，CPU自己重排，只保证最终结果一致；分支预测——你的if-else还没算完，它已经猜好了走哪条路提前开干了；超标量流水线——同时发射多条指令并行执行；SIMD——一条指令同时处理一堆数据。更别提GPU了，上千个核心同时跑同一套指令处理不同的数据，这跟”顺序执行”简直南辕北辙。

IBM在2024年发了一篇研究，标题很直白：冯诺依曼架构正在制约AI计算的能力。核心问题就是那个老生常谈的”冯诺依曼瓶颈”——CPU和内存之间的数据搬运成了最大的性能卡点。AI的workload特点是海量简单计算加海量数据搬运，处理器算得比搬数据快得多，大部分时间就在那儿等数据。所以现在做AI芯片的都在想办法把计算搬到数据旁边去——存内计算、近存计算——这本质上就是在突破冯诺依曼”计算和存储分离”的基本原则。所以你要说冯诺依曼架构至今未变，这话不太准确。**更准确的说法是：冯诺依曼”存储程序”这个核心洞察至今未变——程序本身也是数据，可以被修改、加载、替换——但它的执行模型已经被改到面目全非了。** 你写的代码看起来还是一行一行顺序跑的，那是因为CPU在底下做了大量工作来维持这个”假象”。就好比你以为你在跟一个人聊天，实际上后面是一个团队在同时处理你的消息，只不过给你回的消息是排好序的。

那你可能会问，冯诺依曼都变了这么多了，还有什么是真正没变的？


---


我认为真正没变的，不是某个具体的架构方案，而是那些来自数学和物理的”约束”。这些约束之所以不变，不是因为技术还不够先进，而是因为它们描述的是计算和物理世界的根本限制——就像热力学第二定律，你可以造出效率越来越高的引擎，但你永远造不出效率100%的引擎。

图灵真正伟大的贡献不是图灵机这个具体的计算模型。说实话图灵机从工程角度看是个很笨的设计，一条无限长的纸带来回挪，没人会照着它去造计算机。图灵真正做的事情是证明了**“有些问题是任何计算机都解决不了的”** 。最经典的就是停机问题：不存在一个通用程序，能判断任意程序在任意输入下是否会停止运行。这个结论1936年就有了，快九十年了，量子计算突破了吗？没有。量子计算能做的事情是在某些特定问题上（大数分解、某些搜索和优化问题）比经典计算机快得多，但它不能解决经典计算机解决不了的问题。量子计算可能让你从算一年变成算一秒，但如果一个问题被证明了”不可计算”，量子计算机也算不出来。Church-Turing论题——所有物理上可实现的计算模型，计算能力本质上等价——到2025年依然是学界的主流共识，连量子计算都没有动摇这一点。

这听起来很学术对吧？但这个约束有非常实际的影响。我之前排查过一个静态分析工具的误报问题，当时很困惑：这工具花了大价钱买的，号称多精确多智能，为什么还是会漏报和误报？后来深入了解才明白，**精确的程序分析本身就是不可判定的** ——这不是工具做得不够好，而是数学证明了你不可能造出一个既完全精确又完全覆盖的程序分析器。所有的静态分析工具都在精度和覆盖率之间做trade-off，要么漏掉一些真正的bug（漏报），要么把正常代码标成bug（误报），永远不可能两者都完美。你花再多的钱也买不到”完美的静态分析”，因为它在数学上就不存在。

这就引出了我认为计算机领域最核心、最永恒的东西：**trade-off的必然性。** 


---


我之前写CPU为什么不能靠做大提升性能那篇，本质上讲的也是这个。光刻限制、良率成本、信号延迟、功耗散热，四堵墙同时挡在那里，你攻破一堵其他三堵还在，这就是约束。但当时没展开讲的是，这种”你想要A就必须牺牲一些B”的格局，不是工程师不够聪明找不到两全其美的方案，**而是很多”两全其美”从数学上就被证明了不可能** 。

最经典的例子是CAP定理：分布式系统中，一致性、可用性、分区容错三者最多同时满足两个。这不是谁的经验之谈，这有严格数学证明。你可以设计出三者都”还凑合”的系统——实际上大部分分布式系统都在做这件事——但你不可能设计出三者都”完美”的系统。我在做高并发系统的时候对这点感受很深。当时用Redis做缓存，写入策略选cache-aside还是write-through？选cache-aside，读性能好但有短暂的数据不一致窗口；选write-through，一致性有保证但写入延迟上去了。两种方案各有取舍，不存在”又一致又快”的银弹。你只能根据业务场景选一个”够用”的平衡点。

时间和空间的trade-off更是无处不在。你想查找更快？建索引，拿空间换时间。想省内存？压缩数据，拿时间换空间。想两个都要？有复杂度下界挡着。基于比较的排序，下界是O(n log n)，这不是因为没人找到更好的排序算法，而是数学证明了不可能有更快的。之前我写那篇链表的文章也是类似的结论——链表插入理论上O(1)，数组插入理论上O(n)，但实际跑起来数组反而快四倍，因为cache miss的常数因子太大了。纸面上的复杂度分析只是起点，实测才是终点。但即便实测结果跟理论不同，**那也是因为你的性能模型不够精确，不是因为trade-off消失了——trade-off只是从”操作次数的多少”变成了”缓存命中率的高低”，本质上还是在做取舍。** 

这种trade-off的模式在计算机的每一层都在重复。硬件层有SRAM和DRAM——快的贵、便宜的慢，所以搞多级缓存。芯片工艺进步了五十年，SRAM还是比DRAM快，DRAM还是比SSD快，SSD还是比机械盘快，存储金字塔的层级结构从来没变过，只是每一层都变快了。软件层有一致性和性能——加锁安全但慢，无锁快但极难写对。架构层有耦合和灵活性——拆成微服务灵活了但通信成本上去了。网络带宽五十年涨了几十万倍，但CAP的约束纹丝不动。算法理论发展了七十年，O(n log n)的排序下界还是那个下界。**技术进步让你在约束内的腾挪空间变大了，但约束本身岿然不动。** 


---


还有一个被严重低估的永恒原理：抽象分层。这东西看起来太朴素了，朴素到大家觉得它不算”原理”，但我越写代码越觉得它可能是计算机科学里最核心的思想。

你现在用Python一行代码发个HTTP请求，底下经历了至少七八层抽象：requests库→urllib3→socket→操作系统TCP/IP协议栈→网卡驱动→网卡硬件→物理信号在光纤里跑。每一层只需要知道上下两层的接口约定，不需要关心其他层的细节。这就是为什么你不需要理解电磁波的传播原理就能写Web应用。这个分层模式从1960年代Dijkstra设计THE操作系统开始，六十年了，从来没变过。变的只是每一层的具体实现——协议从TCP到QUIC，存储从机械盘到NVMe SSD，计算从单核到多核到GPU到TPU——但”分层”这个组织原则本身纹丝不动。

为什么？因为**不分层你就管理不了复杂性，而复杂性只增不减** 。现代一个普通的Web请求涉及的软硬件组件可能上百个，不用分层抽象的方式去组织，任何人的大脑都处理不了。但抽象也有它永恒的代价——Joel Spolsky说的”抽象泄漏定律”：所有不平凡的抽象都是有漏洞的。我之前讲过一个@Transactional的踩坑经历，Spring帮你抽象了事务管理，你不需要知道连接池怎么工作，大部分时候这很省心。但某一天一个查询接口里调了外部HTTP，数据库连接在等HTTP返回的几秒钟里一直被占着，高峰期连接池直接被打满。这不是Spring设计得不好，**这是所有抽象的固有特征——它帮你隐藏了复杂性，但隐藏不等于消除，复杂性在底下还活着，某个时刻它就会漏出来咬你一口。** 

这三样东西——可计算性的边界、trade-off的必然性、抽象的泄漏——我觉得才是真正永恒不变的”计算机原理”。它们不变，不是因为技术停滞了，而是因为它们描述的是数学和物理世界的根本特征。量子计算改变了什么？改变了某些问题的效率上限，但没改变可计算性的边界。AI芯片改变了什么？改变了冯诺依曼架构中计算和存储的组织方式，但没有消除数据搬运的物理成本，trade-off还在，只是换了一个平衡点。分布式系统改变了什么？把单机的算力限制打开了，但带来了一致性问题，CAP的约束从天而降。**每一次技术革新都是在约束内找到了新的平衡点，但约束本身从来没有被消除过。** 

所以我一直觉得，学计算机原理最值钱的不是记住冯诺依曼架构长什么样、流水线分几级、中断怎么处理——这些具体知识会过时，十年后你学的那套可能就不是主流了。**真正值钱的是理解每个设计背后的”为什么”：它在做什么trade-off，它受到了什么约束，它放弃了什么换来了什么。** 理解了这些，不管将来出现什么新架构新范式，你都能很快看清它的本质——因为它一定也在那些永恒的约束下做选择，只是选了不同的方向而已。

评论区如果对CAP定理或者可计算性边界这些话题感兴趣可以继续聊，这两块东西都值得单独展开讲讲。