---
创建时间: 2026-02-07
最后修改: 2026-02-15
状态:
  - Inbox
tags: []
---
谷歌 Gemini 准确率从 21% 提升至 97% 仅靠「复制粘贴」，这意味着什么？

谷歌这篇论文我看完了，说实话，看完后我坐在工位上对着屏幕发呆了十分钟，心情很复杂。我们这帮人天天研究怎么优化Attention机制，怎么搞思维链CoT，怎么做各种花哨的对齐训练，结果Google Research现在的结论是，你别整那些没用的，把你那段Prompt复制一遍粘贴在后面，效果最好。

准确率从21%提升到97%，这个数据提升幅度大到不讲道理。如果这是某个不知名的小厂发的PR稿，我绝对当它是造假。但这是Google Research，而且复现成本极低，谁都能去试，他们没必要在这事儿上撒谎。

这背后意味着什么？意味着我们对Transformer架构的理解，可能还停留在盲人摸象的阶段。这也意味着，很多所谓的提示词工程Prompt Engineering，本质上就是在瞎猫碰死耗子，而这次我们终于碰上了一只最大的死耗子。

**为什么简单的复制粘贴能吊打精心设计的思维链？** 

Transformer的核心是注意力机制。我们总以为模型读懂了我们的指令，实际上它只是在计算token之间的概率关联。当你给模型一个复杂的指令，比如要求它遵循五条约束条件去完成一个分类任务，模型在处理过程中，它的注意力是分散的。

输入序列越长，或者模型本身的训练数据分布越强势，它就越容易忽略你当下的指令。这就是为什么我们在实际业务中经常遇到模型偷懒、遗忘指令或者产生幻觉。

现在的做法是搞CoT，让模型一步步思考。这确实有效，尤其是对于推理任务。但Google这篇论文特别鸡贼，它针对的是**非推理任务** 。这非常关键。

什么是非推理任务？就是那些不需要复杂逻辑推演，只需要严格遵循格式、提取信息、或者进行简单分类的任务。这种任务在企业级应用里占比极大，甚至超过80%。

在这些任务里，模型犯错通常不是因为它笨，想不明白，而是因为它**没看清** 或者是**看过了就忘** 。

你把Prompt复制一遍，等于是在物理层面上强行增加了这段指令在Context中的权重。在Self-Attention的计算矩阵里，这段指令出现了两次，它的Key和Value被激活了两次，它对后续生成的Query产生的影响力直接加倍。

这根本不是什么心理学，这就是最朴素的信号处理原理。**信噪比** 。

原来的指令是信号，背景噪音是模型的预训练偏见和其他无关上下文。你把指令说两遍，信号强度翻倍，噪音不变，信噪比直接拉满。模型想看不见都难。

这解释了为什么它在非推理任务上效果这么好。因为非推理任务最怕的就是指令遵循能力差。而对于推理任务，你喊两遍没用，因为它不懂就是不懂，你喊破喉咙它还是不懂。但对于它懂却容易忽略的事，喊两遍就是神技。

**这直接冲击了现有的RAG和Agent开发范式。** 

我们现在做RAG，恨不得把塞进Context里的每一块Token都精打细算。为了省Token，为了不撑爆上下文窗口，我们疯狂做压缩，做摘要。现在Google告诉你，别压缩了，你把关键指令重复一遍，效果比你搞那些复杂的重排算法Re-ranking还要好。

这简直是反直觉的。我们一直以为上下文越短越精炼，模型负担越轻，效果越好。事实可能恰恰相反。对于大模型来说，冗余可能不是一种浪费，而是一种必要的**认知强化** 。

这让我想起人类的沟通。以前我们觉得跟AI说话要像写代码一样精准。现在看来，跟AI说话得像跟听力不好的老人在沟通。重要的事说三遍，这句老话居然在硅谷最前沿的技术里得到了验证。

这也让很多卖课教Prompt Engineering的人很难受。他们教你写各种结构化Prompt，教你设定各种角色Persona，教你用各种复杂的定界符。结果现在最好的技巧是Ctrl+A，Ctrl+C，Ctrl+V。

这也暴露了当前大模型架构的一个缺陷。**注意力机制的分配是不可控的** 。我们无法显式地告诉模型：这句话很重要，你给我死死记住，权重加100。我们做不到。我们只能通过这种笨办法，通过增加出现的频率，来骗过模型的注意力机制，让它被动地分配更多权重。

这其实挺可悲的。我们造出了这么强大的智能体，控制它的手段却如此原始。

**从21%到97%的跃升，揭示了模型性能的虚假繁荣** 

这才是最让我细思极恐的地方。

如果在不重复Prompt的情况下，准确率只有21%，那说明这个模型在原始状态下基本是不可用的。21%的准确率在工业界就是个笑话。

这意味着，我们之前看到的很多模型的高分表现，可能都是极度依赖特定Prompt诱导的结果。模型的原生能力并没有我们想象的那么强。它的鲁棒性极差。稍微改动一下指令，或者指令稍微长一点，它的性能就崩塌到了21%。

我们正在构建的应用大厦，地基可能比我们以为的要松软得多。

这也解释了为什么很多企业落地大模型的时候觉得效果差。因为在实验室里，研究员会反复调试Prompt直到跑出SOTA数据。但到了真实业务场景，用户或者业务系统生成的Prompt是千奇百怪的，没有那层保护色，模型立刻现了原形。

Google这个技巧，实际上是一种**补丁** 。它修补了模型在长上下文和复杂指令下注意力弥散的Bug。但它治标不治本。

很多人说这是免费午餐，不影响生成速度。这点我要纠正一下。

Transformer的推理成本主要由两部分组成：Prefill（预填充）和Decoding（解码）。

你把Prompt重复一遍，输入的Token数量确实翻倍了。这意味着Prefill阶段的计算量增加了。虽然Decoding阶段（生成答案）的速度不会变慢，甚至可能因为一次做对而变相变快了（减少了重试），但输入端的成本是实实在在增加了。

在DeepSeek、GPT-5这种长文本模型普及的今天，Token价格虽然被打下来了，但如果你在这个基础上每次都搞Prompt Repetition，对于高并发的业务来说，成本压力还是存在的。

但这笔账算下来极其划算。

如果是为了提升那70%多的准确率，多花一倍的Input Token钱，简直是血赚。在商业逻辑里，准确率从20%提升到90%是质变，是从**不可用** 到**商用** 的跨越。为了这个跨越，算力成本增加一倍根本不是问题。

**未来这种Prompt Repetition可能会被固化到底层框架里。** 

以后的开发框架，比如LangChain的后继者们，可能不需要开发者手动去复制粘贴。中间件会自动识别关键指令，并在发送给模型之前自动在系统层面做一次重复注入。这对开发者是透明的。

甚至模型厂商在训练阶段就会引入这种机制。现在的SFT（监督微调）数据大多是精炼的问答对。未来可能会专门加入这种重复性指令的数据，让模型学会：只要是用户强调的内容，或者是出现在特定位置的内容，就要给予超级权重。

或者，我们最终会修改Attention机制本身。现在的Attention是平权的，虽然有位置编码，但依然很容易被淹没。未来的架构可能会引入**显式记忆模块** ，允许我们将关键指令写入一个不随上下文滑动而衰减的记忆区。

但在那一天到来之前，老老实实地复读，就是最高效的手段。

别再迷信那些花里胡哨的Prompt框架了。什么少样本学习Few-Shot，什么思维树ToT。在绝对的信号强度面前，技巧都是苍白的。

如果你现在的业务场景里，模型经常忽略指令，经常格式错误，经常漏掉关键信息。别去调温度参数了，别去改系统提示词了。简单点，把你最在乎的那几条要求，在Prompt的最后面，再完整地把粘一遍。

甚至可以更极端一点。我最近在测试中发现，对于特别顽固的模型，重复三遍的效果比两遍还好，虽然边际收益在递减，但在极度复杂的长文中，三遍几乎能保证100%的遵循。

这听起来很土，一点都不高科技。但搞工程就是这样，不管黑猫白猫，能抓老鼠就是好猫。

我们现在处于一个很尴尬的阶段。我们手里拿着核武器级别的算力，却还在用最原始的咒语来控制它。这种**重复咒语** 有效，恰恰说明了我们对智能本质的理解还非常肤浅。

大模型不是人，它没有尊严，它不觉得你啰嗦。它只是一个概率分布机。你给它的确定性信号越多，它吐出来的结果就越收敛。

这个技巧的流行，可能会让很多做微调Fine-tuning的公司倒闭。以前大家觉得模型不听话是因为没微调好，现在发现原来只是没听清。既然多喊一声就能解决，谁还愿意花几十万去租卡训练LoRA呢？

完全可以将任何涉及到结构化输出（比如这就要求JSON格式）、任何涉及到负面约束（比如严禁出现某某词）、任何涉及到多条件判断的任务。

把这些核心指令提取出来，做一个变量，在Prompt构建的最后一步，再次追加进去。

甚至都不需要改动原来的Prompt结构，就是在User Input之前，强行插入一段：**请注意，请务必严格遵守以下指令：[重复一遍核心指令]** 。

就这么一个动作，你的系统鲁棒性可能会直接上一个台阶。

这个世界有时候就是这么荒谬。我们以为通往AGI的道路是由复杂的数学公式铺成的，结果路边立个牌子，上面写着：**重要的事情说三遍** 。

这不仅仅是技术的胜利，这是经验主义的胜利。它提醒我们，面对这种不可解释的黑盒模型，实证比理论更重要。去试，去跑数据，别光听理论推导。

这就是现状。21%到97%，中间差的不是智商，是重复。

这也让我对人类的本质产生了怀疑。我们人类自己，不也是这样吗？很多道理听一遍记不住，非得社会毒打几遍，或者父母唠叨几百遍，最后才刻在脑子里。

原来Transformer和人类大脑，在**复读机** 这个属性上，达成了惊人的一致。