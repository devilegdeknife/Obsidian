---
title: "数据分析"
status: inbox
source: note
created: 2026-02-02
tags:
  - inbox
para: inbox
updated: 2026-02-11
---
如果让我挑一个让所有初入行的Data Scientist最搞不清的概念，绝对是维度诅咒（Curse of Dimensionality）。
大多数人的几何直觉停留在三维空间。你觉得数据点是均匀分布的，距离是有意义的。但当你处理风控模型（动辄几百个特征变量）或者基因组数据（上万个维度）时，你的欧几里得直觉会把你带进沟里。
在高维空间里，几乎所有的点都位于超球体的表面（Shell）上，球心是空的，且所有点之间的距离几乎相等。
听起来很玄吗？想象一个𝑑d维的超立方体，里面内切一个超球体。随着维度𝑑d增加，超球体的体积占超立方体体积的比例会迅速趋近于零。你可以去翻翻The Elements of Statistical Learning，Hastie那帮人写得很清楚：在10维空间里，大部分数据点都已经跑到了边缘。
如果你想看懂 Hastie 他们是怎么用数学语言描述这种空旷的，可以去看一下 斯坦福经典《统计学习要素》（ESL）的中文翻译配合代码实现，特别是关于高维问题和偏差-方差权衡的章节，能让你明白为什么简单的模型在高维空间反而更有效。
这意味着什么？
如果你在做一个高维的用户画像聚类，或者试图在高维特征空间里找“最近邻”（Nearest Neighbor）来做推荐系统，你会发现所有的样本对你来说都是“等距离”的。在这个空间里，没有邻居，大家都是孤岛。
这就是为什么我们在量化挖掘因子时，必须极其克制地使用特征数量。甚至在某些时候，必须要用Lasso或者Ridge去做正则化，强行把维度降下来。不是为了好看，而是因为在高维空间里，所谓的“数据密度”根本不存在。你以为你有大数据，但在那个维度下，你的数据稀疏得像撒哈拉沙漠里的一把沙子。
很多风控模型在回测时表现完美，一上线就崩盘，往往就是因为模型在拟合高维空间边缘的噪声，而不是核心的逻辑。

第二个这样的定理我会提名，詹姆斯-斯坦估计（James-Stein Estimator）。
在统计推断的经典教材里，样本均值是总体均值最好的无偏估计量。这符合我们的直觉：如果你想估计十支不同股票的未来收益率，最好的办法当然是分别计算这十支股票的历史平均收益率。对吧？
错。而且是大错特错。
早在1956年，Charles Stein就证明了一个震惊学界的现象：当维度大于等于3时，样本均值不再是“最佳”的估计量（Inadmissible）。
你可以构造一个更有偏的估计量（比如把所有的均值向某个任意常数收缩，Shrinkage），它的均方误差（MSE）竟然比样本均值更小。
这意味着什么？这简直是统计学上的“量子纠缠”。
想象一下，你在估计三件事：北京明天的降雨量、腾讯明年的股价、以及我今晚吃几个饺子。这三件事风马牛不相及，相互独立。按照常理，你应该分别收集数据，分别估计。
但是James-Stein定理告诉你：如果你把这三个不相干的数据放在一起，算一个总的收缩估计值，居然能从数学上证明，你的整体预测误差会变小。
这简直就是魔法。
在现实的量化投资组合优化中，这个定理是我们的救命稻草。如果我们单独估计每只股票的Beta值，误差会大得离谱，因为市场充满了噪声。利用James-Stein的思想（或者贝叶斯分层模型），我们将所有股票的Beta值向市场平均值进行“收缩”。虽然我们在每只股票的估计上引入了偏差（Bias），但我们极大地降低了方差（Variance）。
这背后的哲学含义残酷而深刻：在充满噪声的世界里，追求个体的“无偏”和“精准”是徒劳的。只有承认个体的平庸，向集体靠拢，才能在整体上获得生存优势。 所谓的个性化预测，往往不如这一刀切的修正来得稳健。
Efron有一篇非常精彩的文章 Stein’s Paradox in Statistics，用棒球选手的打击率做例子，读完你会对“平均”这个词产生敬畏。

然后是随机游走中的反正弦定律（Arcsine Law）。
如果你去澳门或者拉斯维加斯，或者盯着分钟级别的K线图看，你会产生一种错觉：如果一个游戏是公平的（比如抛硬币，期望为0），那么输赢的状态应该像拉锯战一样，频繁地在0轴上下波动，在这个半场待一会儿，再去那个半场待一会儿。
如果一个赌徒连赢了很长时间，你一定会觉得他“手气红”或者“有技术”；如果一只股票连续位于均线切上方很久，你会觉得这是“多头趋势”。
Feller在那本神一般的 An Introduction to Probability Theory and Its Applications 第一卷里，用反正弦定律冷冷地粉碎了这个直觉。
对于一个完美的对称随机游走（公平赌局），如果你抛硬币2N次，这一串序列停留在正半轴（处于盈利状态）的时间比例，并不是集中在50%，而是集中在0和1两端！
这是一个U型分布。也就是说，在一个完全随机、完全公平的游戏中，最常见的情况不是你赢一会输一会，而是一旦你运气好在这个半场，你可能这辈子绝大部分时间都待在这个半场；或者一旦你运气不好陷入亏损，你可能哪怕玩到地老天荒，绝大部分时间都在回本的路上。
这就是为什么大多数散户在股市里感觉那么痛苦。哪怕市场是有效的、随机的，根据反正弦定律，一旦你被套住，你可能会在那个“水下”的状态停留极长的时间，长到你的心理防线崩溃，长到你相信“庄家在搞我”。
这也是为什么我们在做高频策略回测时，极其警惕那种“长时间盈利”的策略。如果一个策略的PNL（盈亏）曲线长期在水上不回头，如果不考虑交易成本和滑点，这不一定是Alpha，这可能仅仅是随机游走的幸存者偏差。
趋势往往是随机性伪装的秩序。 绝大多数人眼里的“技术形态”，不过是反正弦定律开的一个玩笑。

还有遍历性破缺（Ergodicity Breaking）：平均值的陷阱
这是最近几年在经济物理学和复杂的风险管理中被反复提及，却依然被主流经济学教材无视的概念。
很多人喜欢用“期望值”来做决策。比如我给你一个赌局：50%的概率你的资产翻倍（乘以2），50%的概率你的资产减半（乘以0.5）。
很多受过基础统计教育的人会算：期望收益率 = 0.5×2+0.5×0.5=1.250.5 \times 2 + 0.5 \times 0.5 = 1.25。 哇，期望收益是25%！这是一笔好买卖！只要我玩得次数够多，根据大数定律，我肯定发财！
你要是真敢满仓去玩这个游戏，你的资产归零只是时间问题。
为什么？因为集合平均（Ensemble Average）不等于时间平均（Time Average）。 这就是遍历性的破缺。
在这个游戏中，虽然针对“一群人”玩一把的期望是赚钱的；但对于“你一个人”连续玩下去，你的资金变化是乘法性质的。玩两把后，你的资金变成了 2×0.5=12 \times 0.5 = 1。不赚不赔？那是中位数。 长期来看，这个过程的增长率由几何平均数决定，2×0.5=1\sqrt{2 \times 0.5} = 1。 如果你引入一点点交易成本或者稍微调低一点赔率（比如输了乘以0.4），算术平均值可能还是正的，但几何平均值变成了负的。
在时间的长河里，只要你一直在桌上，大数定律不会救你，它会把你推向几何平均的必然结局。
这就是为什么凯利公式（Kelly Criterion）是赌徒和交易员的圣经，而不是什么马科维茨的均值方差模型。凯利公式优化的不是算术期望，而是对数财富的期望增长率（Growth Rate）。
在现实世界里，如果你不懂遍历性破缺，你就会拿你在商学院学的DCF模型去估值一家初创公司，算出一个漂亮的期望估值，然后投入重金，最后发现自己破产了。因为你没有意识到，你只有一条命，你无法在这个宇宙里平行重复一万次你的人生来取那个“期望值”。
对于任何带有乘法效应（复利）和吸收壁（破产即出局）的系统，算术平均值就是一张昂贵的废纸。

最后我要提名的是尾部风险与极值理论。
2008年金融危机前，华尔街满大街都是用高斯Copula模型算CDO定价的人。大家都假设违约概率服从正态分布，或者只不过是稍微胖一点的分布。
我当时还是个刚入行的菜鸟，看着那些模型算出来的VaR（在险价值），显示我们要担心的最大风险不过是每天波动个3%。
然后市场就崩了。按照正态分布，那天发生的波动幅度，理论上在宇宙诞生到现在都不应该发生一次。但它就是发生了，而且紧接着第二天又发生了一次。
这里就要提到极值理论（Extreme Value Theory, EVT）和幂律分布（Power Law）。
在自然界和金融界，极端事件发生的概率，并不像正态分布那样指数级衰减，而是以幂律衰减。意思是，那个“黑天鹅”比你想象的要大得多，也要频繁得多。
Taleb在 The Black Swan 里骂得很难听，但他骂得对。而在数学上，你需要去读Embrechts, Klüppelberg, and Mikosch写的 Modelling Extremal Events for Insurance and Finance。
最颠覆三观的是：对于某些厚尾分布，方差甚至是无穷大的。
如果你面对的是一个方差无穷大的分布（比如自由度很低的t分布或帕累托分布），你所学的中心极限定理（CLT）虽然在理论上依然指向某种稳定分布（Levy Stable Distribution），但在实践层面，基于方差的标准差、夏普比率、相关系数全部失效。
如果你觉得这些概念太抽象，想从基础补起，我不建议你去看传统的那种教材，可以试试这个配套了可视化的 《鸢尾花书》（配套可视化代码），对于理解这种反直觉的数学特性，图形往往比公式更直观。
你在样本里测算出的波动率，会随着时间的推移越来越大，永远不收敛。你以为你控制了风险，其实你只是在一个火药桶上抽烟，而且火药桶的引线长度是随机的。
这在保险精算和再保险领域是常识，但在很多互联网公司做A/B Test或者流量预测时，依然有人天真地用正态分布的3-sigma准则去剔除所谓的“异常点”。那些你删掉的异常点，可能才是真实世界的本体。
说了这么多，如果你还没被吓跑，说明你适合研究这个。
统计学里这些反直觉的定理，归根结底是在告诉我们一件事：人类的大脑是为因果律和线性关系进化的，不是为概率和高维空间进化的。
我们渴望确定性，渴望回归均值，渴望付出就有回报（相关性）。但数学冷冷地告诉我们： 维度是诅咒，让相似性消散； 个体会失效，必须依赖集体收缩； 公平的游戏里充满了长期的赢家和输家； 期望值为正的赌局可能让你倾家荡产； 而那个足以毁灭你的极端事件，正潜伏在厚厚的尾部里，嘲笑你的3-sigma风控模型。
这就是为什么我特别推荐去读一读 Pearl 的《为什么》（The Book of Why），或者上手这本 《Causal Inference for the Brave and True》（配套习题）。如果不搞懂因果推断（Causal Inference），你会永远被数据表面的相关性欺骗，把反正弦定律制造的随机趋势当成因果逻辑。
作为一名从业者，我不相信模型能预测未来。我只相信模型能帮我理解：在这个混乱的随机过程中，我现在的仓位到底处于什么位置，以及我离破产还有多远。这就是统计学最残酷也最迷人的地方：它不给你答案，它只给你边界。在这个边界之内，你可以尽情地用你的直觉去赌；但一旦越过边界，上帝就不会再掷骰子了，他会直接掀桌子。
去读读E.T. Jaynes的 Probability Theory: The Logic of Science 吧，如果C&B让你觉得只是在玩弄符号，Jaynes会教你如何像物理学家一样思考概率的本质。那时候你会明白，概率不是频率，而是信息的缺憾。