---
type: resource
source: zhihu
url: https://www.zhihu.com/question/621649777/answer/1949172358603608940
topics: [AI, AI绘画, 机器学习, 模型训练, 工程, 基础设施]
status: processed
para: resource
tags: []
created: 2026-02-02
updated: 2026-02-11
---
# 深度学习顶会，为什么公开数据集，代码，也跑不出论文的结果？
[内容链接](https://www.zhihu.com/question/621649777/answer/1949172358603608940)

有一种可能是，你的“硬件条件”跟人家相差太远，导致有些优势被掩盖了，一个专为1000公里续航的电跑车设计的电源优化方案，一台续航201km的代步车即使“上”了，也看不出“好”来。

每次我在顶会论文上看到诸如“成功将训练时间从数小时甚至数天缩短到几分钟”之类的创新，就忍不住嗤之以鼻：

切，你们在那么“高端”的卡上实现效率提升，有啥用？啊？有啥用！我又没有那么好的卡！！！

不过好歹我还拥有一块祖传的3090，据我了解，很多研究生所在的课题组可能只有一块魔改版的2080ti，还要“排队”等机时……

假期我去找导师聊天，跟导师抱怨算力不够用的问题，导师发出了一个灵魂拷问：“你为啥不试试云算力呢？”

诶，有道理欸！

现在有那么多拥有H100的算力租赁平台，我为啥不试试呢？

说干就干，我选了一篇从2022年就让我心心念念想要复现的论文：

Instant Neural Graphics Primitives with a Multiresolution Hash Encoding

![图片描述](https://pic1.zhimg.com/v2-3a503ca1dea424e3ef0baf20b324e110_r.jpg?source=2c26e567)

Instant-NGP提出的多分辨率哈希网格编码结合小型全融合MLP的方案，把传统NeRF等神经图形任务的训练时间从数小时甚至数天缩短到分钟级别。

从复现的角度来说，这篇论文的技术路线非常清晰，从哈希函数设计到多分辨率层次构建，再到网络架构选择，每个步骤都有详细的参数设置和实现指导。

项目主要涉及CUDA编程和基础的深度学习框架，不需要过于复杂的外部库以及特殊硬件配置。

而且吧，这个项目的结果具有直观的可视化效果，不仅可以通过PSNR和SSIM等量化指标评估重建质量，还可以通过对比视频直观的观察训练效果。

先说结论：

我租了一张云端H100卡，中间犯了很多可以改正的错误，一共用了不到10个小时的机时（其实完全不用浪费这么多机时），成功复现了上面图中最后一行，乐高斗车的那个结果。

先看看我做出来的对比视频：

[https://www.zhihu.com/video/1949167187874017943](https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1949167187874017943)

以及跑了3w多步的PSNR和SSIM结果：

![图片描述](https://pic1.zhimg.com/50/v2-9d06089d7acf52985127892d362a13d9_720w.jpg?source=2c26e567)

接下来我写了一个保姆级的复现教程，有兴趣的同学可以自己复现一下试一试。

一般来说，云算力平台通常会有一个快捷编程工具，比如JupyterLab：

![图片描述](https://picx.zhimg.com/v2-f7033583acc3a1cbafbe877cd7db60c1_r.jpg?source=2c26e567)

但是如果我们想要复现一个稍微有一点复杂的项目的话，我建议还是采取本地写、云端跑的方式，既可以用自己熟悉的编程工具，又比较稳定。

我选择的是VS code，先安装Remote Development扩展包：

![图片描述](https://pic1.zhimg.com/v2-9387c525ce1b8bbe5eade67d3f337466_r.jpg?source=2c26e567)

然后打开powershell，在你的用户目录下创建 C:\Users\<你的用户名>\.ssh\config，并写入配置：

![图片描述](https://picx.zhimg.com/v2-db4b55162affe62b3e4236f5a8e3d27c_r.jpg?source=2c26e567)

这里需要注意，如果在云端进行了“关机”操作后，再次“开机”的时候，那个端口号会发生变化，就是Port后面的5个数字会变得不一样。

所以，每次重新开机后，一定要记得修改config文件中的端口号。

用记事本打开即可，但是不要给这个文件加后缀：

![图片描述](https://picx.zhimg.com/50/v2-98b2eedb7001208ec6b6c6b06c284425_720w.jpg?source=2c26e567)

接下来测试一下配置是否生效：

ssh -v gpulink

出现这些日志，说明配置生效了：

![图片描述](https://picx.zhimg.com/v2-cff6a4e502c13ef8cd237d2f00719410_r.jpg?source=2c26e567)

回到VS code，按F1，输入输入并选择“Remote-SSH: Open SSH Configuration File…”，选中C:\Users\<你的用户名>\.ssh\config，在弹出的列表里选择**gpulink**连接即可。

从刚才开始就经常出现的这个**gpulink**，其实就是我选择的云算力平台：

[https://gpulink.cc?invite_code=X3n2Pa](https://link.zhihu.com/?target=https%3A//gpulink.cc/%3Finvite_code%3DX3n2Pa)

我选择这个平台，主要是看上了它很“壕”，有很多块H100，而且目前空余算力很充足：

![图片描述](https://picx.zhimg.com/v2-34aea55c79f33a52756be62b149d9e32_r.jpg?source=2c26e567)

价格是RTX4090，1.88/小时，H100，15块钱/小时。

**H100略微有一点点小贵，但是它现在注册就送100块钱的算力券，转介绍还可以再给50块钱算力券，所以我很“豪横”的直接租了块H100。**

选择了gpulink 连接之后，按照提示输入密码就可以成功连接到服务器了。

接下来，我们就可以在服务器路径下创建文件，正式开始准备项目复现。

在服务器中创建路径：

mkdir -p /root/workspace/ngp-repro/remote

考虑到有些同学可能不太会用VS code，这里稍微多说两句。

按Ctrl+`,打开TERMINAL：

![图片描述](https://pica.zhimg.com/v2-ffc2f1d12ea0dd14d761bad5ded7f006_r.jpg?source=2c26e567)

上面那句创建路径的命令写在这里即可。

接下来配置环境，创建env.sh文件：

![图片描述](https://picx.zhimg.com/v2-43985566affd72c6ee67390f97985a7a_r.jpg?source=2c26e567)

这里需要注意的是，右下角状态栏把 Plain Text 切换为 Shell Script（语法高亮），把 CRLF 改成 LF（不过服务器端默认就是LF），保存时确保编码是 UTF‑8。

![图片描述](https://pica.zhimg.com/50/v2-b608a1929b45b52eca98031bd2baef50_720w.jpg?source=2c26e567)

然后是clone仓库：

![图片描述](https://picx.zhimg.com/v2-f83bfab2d630ea1ff5c379ec0b0ddabc_r.jpg?source=2c26e567)

我在clone的时候，速度还可以，所以我就直接克隆了。如果心疼机时的话，可以考虑手动下载zip，然后上传导入，速度会比直接克隆快一点。

接下来构建一个名为train_eval_scene_py.sh的文件，**实现整条复现实验流水线一条命令跑通。**

先说这个文件的用法：

bash /root/workspace/ngp-repro/remote/train_eval_scene_py.sh <scene名或绝对路径> [训练步数]

比如我要做lego那个案例：

bash /root/workspace/ngp-repro/remote/train_eval_scene_py.sh /root/datasets/lego 30000

然后咱们说这个文件的具体构成。

首先是参数及其默认值：

![图片描述](https://pic1.zhimg.com/v2-975ab7555b43d19a88d7c418e7789b2c_r.jpg?source=2c26e567)

然后是数据路径解析：

![图片描述](https://picx.zhimg.com/v2-d9cb59acac956c9cf48a5f1e82eced45_r.jpg?source=2c26e567)

以及数据完整性自检以及统计png文件的数量：

![图片描述](https://picx.zhimg.com/v2-2fa8ffc8d50e73524f642b1396e21b31_r.jpg?source=2c26e567)

“自检”要求数据中必须有transforms_train.json与transforms_test.json。

其实吧，如果直接从官方拖案例数据的话，不检也行，反正那个数据肯定是完整的。

但是吧，这句是我踩了个坑之后特意加上的。

众所周知，直接从GitHub上拖数据，一字可谓之“慢”，俩字“很慢”，仨字“非常慢”！

要是平时，考虑到顶多费点电钱，我也就慢慢拖了。

可是，现在我用的是15块钱/小时的算力啊！数据拖上3个小时，那就是45块钱，4个小时就是60块钱啊！太贵了有木有！

一开始我没意识到这个问题，写好代码，运行上就去洗澡了。

等过了将近1个小时，我回来看到6%的进度条时，再看到少了15块钱的账户时，我赶紧点了关机，然后老老实实的去kaggle下载数据集了：

[https://www.kaggle.com/datasets/nguyenhung1903/nerf-synthetic-dataset](https://link.zhihu.com/?target=https%3A//www.kaggle.com/datasets/nguyenhung1903/nerf-synthetic-dataset)

只是在下到这个真·数据集之前，我在Huggingface上下到个“假”的，那个同名的数据集里只有一些运行结果，我费劲巴拉的上传到服务器之后，发现根本不能跑，还白瞎了20分钟的机时……

言归正传，继续往下说，接下来设置输出目录：

![图片描述](https://picx.zhimg.com/v2-037a45fb15d7a260108b698570f8a824_r.jpg?source=2c26e567)

依赖项检查：

![图片描述](https://picx.zhimg.com/v2-fb6a19944e202629e821245f1cdd6eaa_r.jpg?source=2c26e567)

强制使用系统 /usr/bin/cmake，避免 conda cmake 的 policy 冲突：

![图片描述](https://pica.zhimg.com/v2-79ed940d4ea6a67857af08df81231091_r.jpg?source=2c26e567)

训练，可以跳过并加载已有快照：

![图片描述](https://picx.zhimg.com/v2-4a3fb51e84632023a84e318f08914df2_r.jpg?source=2c26e567)

下面加一句这个：

export DATA_PATH OUT_DIR RENDER_DIR GT_DIR SNAPSHOT

因为我在调试过程中，遇到过KeyError: 'DATA_PATH'问题。

渲染测试集：

![图片描述](https://picx.zhimg.com/v2-26d85e9a7eef4b72370efc4a4f7a3c85_r.jpg?source=2c26e567)

显式 --width/--height + --screenshot_spp，避免分辨率缺失和噪点问题。

复制 GT 帧，保持与test数据集中的照片的位姿顺序一致：

![图片描述](https://pica.zhimg.com/v2-d9482467013e650d6664795e9dd22064_r.jpg?source=2c26e567)

最后计算 PSNR/SSIM：

![图片描述](https://picx.zhimg.com/v2-5b8de826c1faf5964b75692a3ad8d901_r.jpg?source=2c26e567)

按数字提取排序键，避免按文本排序，确保 r_2.png 在 r_10.png 之前。

帧大小不一致时自动双三次重采样对齐，结果输出为metrics.csv。

合成视频：

![图片描述](https://picx.zhimg.com/v2-b89c34ef9e098d4bf0569e4a8f8d784e_r.jpg?source=2c26e567)

分别利用模型预测得到的渲染图像生成渲染视频，test中的真实照片生成GT（Ground Truth）视频，以及二者合成的对比视频。

虽然说最重要的脚本我们已经准备好了，但是别忘了，我们前面还“手动”下载了一个数据集，别忘了把它导入：

![图片描述](https://picx.zhimg.com/v2-37fc0559f1c174171fa77ab7bee5b2eb_r.jpg?source=2c26e567)

把这些脚本都保存好后，统一赋权：

chmod +x /root/workspace/ngp-repro/remote/*.sh

然后按照前面提到的使用方法运行train_eval_scene_py.sh，见证“奇迹”的时刻到了：

![图片描述](https://pica.zhimg.com/v2-cd7e7722b1765ef2e3107e1e92f8e471_r.jpg?source=2c26e567)

选择lego数据集，训练30000步，仅仅1分25秒就完成了训练有木有！

不愧是H100有木有！

如果有谁不信邪的话，你可以用实验室祖传的魔改2080ti试一下……

我的这次云端复现，因为是我第一次使用云算力，所有有些步骤没有想明白，浪费了一些机时。

不过即便如此，gpulink注册时送的算力券也足够让我成功复现了lego，chair，hotdog，ship,drums等几个案例，亲眼见证了“**成功将训练时间从数小时甚至数天缩短到几分钟**”。

这次云端H100复现Instant-NGP的经历，让我对“算力就是生产力”这句话有了更深的理解。从传统NeRF需要数小时甚至数天的训练时间，到在gpulink的H100上1分25秒就能完成30000步训练，这种质的飞跃真的让人震撼。

回想起我刚开始看到这篇论文时的那句“切，你们在那么'高端'的卡上实现效率提升，有啥用？”，现在想来还挺打脸的。gpulink这种云算力平台的出现，确实让我们这些“普通玩家”也能体验到顶级硬件的威力。

虽然H100的15块钱/小时初看起来不便宜，但考虑到时间成本和复现效果，这个投入还是很值得的。更何况平台还很“壕”的送了算力券，基本上够我把整个NeRF Synthetic数据集的8个场景都跑一遍了。

而且平台足够稳定，从SSH连接到VS Code远程开发，整个流程都很顺畅，没有出现掉线或者卡顿的情况。

H100的空余算力很充足，不用担心抢不到卡的问题。

平台的计费很透明，用多少算多少，关机就停止计费，不会有什么隐藏消费。看看我的消费记录，就知道我有多会“过日子”了：

![图片描述](https://picx.zhimg.com/v2-488c8830e2225078f6f10bd9d4e11f84_r.jpg?source=2c26e567)

对于想要从顶会论文找“灵感”的同学来说，与其在实验室排队等那张祖传的魔改2080ti，还要担心被师兄师姐催着让卡，不如直接上云端体验一下什么叫“算力自由”。

规划好算力使用，把该下载的提前下载好，云算力的价钱真的可以接受。而且，平台还有2块钱1小时的4090呢不是？

复现感兴趣的项目，找到灵感发几篇好文章，顺利毕业拿offer，一切也就都值得了，不是吗？
