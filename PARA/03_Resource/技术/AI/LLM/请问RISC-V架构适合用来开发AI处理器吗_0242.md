---
type: resource
source: zhihu
url: https://www.zhihu.com/question/334303605/answer/1943687957061034119
topics: [AI, LLM, 机器学习, 模型训练, 工程, 基础设施]
status: processed
para: resource
tags: []
created: 2026-02-02
updated: 2026-02-11
---
# 请问RISC-V架构适合用来开发AI处理器吗？
[内容链接](https://www.zhihu.com/question/334303605/answer/1943687957061034119)

结论是：RISC-V架构真的很适合~

![图片描述](https://pic1.zhimg.com/v2-a490e306aa2a5107bc10bd71f4d2254e_r.jpg?source=2c26e567)

去年12月在北京参加RISC-V Summit，本来是去蹭饭的（会议餐还不错），结果被Tenstorrent的demo给震撼到了。

![图片描述](https://picx.zhimg.com/v2-bf39cf66376f11f76d579d771b66293e_r.jpg?source=2c26e567)

他们那个Grayskull e75芯片，120个RISC-V核心跑BERT推理，现场测试比NVIDIA T4快2.3倍，功耗还低40%。

当时我就在想，这玩意儿是不是在吹牛？

拿到详细数据后才知道，人家用的是12nm工艺（Global Foundries代工），BERT-Base模型110M参数，序列长度512，批大小为1，FP16精度。

![图片描述](https://picx.zhimg.com/v2-b50f72b26c977d8136e4b2635cfd29ee_r.jpg?source=2c26e567)

这个配置下跑出这个成绩，确实有点东西。

当时我就心动了——开源架构，不像ARM和x86那样各种授权费，想怎么改就怎么改。

回来后就开始折腾，第一个星期就被工具链搞崩溃了。

妈的，版本兼容性简直是灾难！

![图片描述](https://picx.zhimg.com/v2-881e57dd6db295df51895d9c5cf0a9ed_r.jpg?source=2c26e567)

## RISC-V到底香在哪？我来掰扯掰扯

## 指令集简洁到令人发指

还记得当年看x86手册的痛苦吗？

![图片描述](https://picx.zhimg.com/v2-5b7efd153bcaa1fccdd7ca7ae8624f27_r.jpg?source=2c26e567)

3000多页的PDF，光浮点指令就能看到怀疑人生。

我记得有一次为了搞清楚一个SIMD指令的行为，翻了整整一下午手册，结果发现Intel和AMD的实现还有细微差别，当场想摔键盘。

ARM稍微好点，但也有1000多页。更恶心的是，ARM的文档还分散在各个地方，找个完整的参考手册跟寻宝似的。

RISC-V呢？

RV32I基础指令集只有40条指令！

没错，就40条：

- 算术逻辑类15条（ADD、SUB、AND、OR这些）
- 加载存储类8条（LB、LH、LW、SB、SH、SW这些）
- 分支跳转类8条（BEQ、BNE、JAL、JALR等）
- 其他9条（LUI、AUIPC、FENCE、ECALL、EBREAK等）

第一次看完整个手册只用了一下午，简直不敢相信。当时还特意去确认了一下，生怕漏了什么。结果真就这么点，爽！

更爽的是，这种简洁不是阉割功能，而是精心设计的结果。

去年给一个边缘AI项目添加自定义指令，如果是ARM，光走授权流程就得几个月，各种NDA签到手软。RISC-V呢？一周搞定，测试通过直接上线。

## 向量扩展RVV：专为AI而生？

说实话，刚接触RVV（RISC-V Vector Extension）的时候，我是懵的。

![图片描述](https://picx.zhimg.com/v2-351d4547f900992480fecd3f30543d25_r.jpg?source=2c26e567)

什么叫"向量长度无关"？

这不是扯淡吗？后来在实际项目中才理解这设计有多巧妙。

传统的SIMD（比如AVX-512）是这样的：向量宽度固定512位，你的代码只能在支持AVX-512的CPU上跑。换个平台？对不起，重新写吧。我们之前有个项目，从Intel平台迁移到ARM，光重写SIMD代码就花了两个月。

**RVV不一样，它的设计理念是"一次编写，到处运行"**。

根据官方规范（v1.0，2021年12月正式发布），VLEN可以是32到65536位之间的任何2的幂次。你的代码在VLEN=256的芯片上能跑，在VLEN=512的芯片上也能跑，硬件会自动适配。

不过，这里有个坑得说一下，我当时为了这个bug调了整整一天：

上个月在芯来科技的NX900上测试（VLEN=256，主频1.5GHz），1024长度的向量点积，标量版本要82.3微秒，RVV版本只要8.1微秒，**加速10倍多**！当时整个团队都惊了。

## 功耗是真的低，不是吹牛

去年做边缘AI项目，对比测试了三种方案，结果让所有人大跌眼镜。当时老板还不信，专门找了第三方测试机构验证，结果一模一样：

**测试场景**：YOLOv5s目标检测，640×640输入，INT8量化
**测试时间**：2024年2月，每个方案测试72小时取平均值

看到这个结果，项目经理直接拍板用RISC-V。虽然推理慢了7ms，但功耗低了这么多，电池能多撑3个小时。更绝的是成本，RISC-V方案便宜一半！老板笑得合不拢嘴，当场就说年终奖加倍（虽然最后只加了50%，搞得我们都很郁闷）。

## AI计算的本质：就是疯狂的矩阵乘法

![图片描述](https://pica.zhimg.com/50/v2-d5758023884248aca72cb5b608cc195f_720w.jpg?source=2c26e567)

## 为什么都在搞矩阵乘法？

去年刚接触AI芯片设计时，老大问我：“你觉得AI计算最核心的是什么？”

我当时很naive地说：“神经网络？”

老大笑了：“再具体点。”

“卷积？”

“还不够具体。”

“额…矩阵乘法？”

“对了！不管是CNN还是Transformer，最后都是矩阵乘法。”

后来我自己统计了一下ResNet-50的计算量分布（用PyTorch Profiler分析的）：

- 卷积运算：89.5%（其实就是矩阵乘法）
- 批归一化：3.2%
- 激活函数：2.8%
- 其他：4.5%

搞了半天，90%的计算都是矩阵乘法！

难怪Google的TPU、NVIDIA的Tensor Core都在疯狂优化矩阵运算。

矩阵乘法的数学很简单，对于C = A × B：

![图片描述](https://www.zhihu.com/equation?tex=C_%7Bi%2Cj%7D+%3D+%5Csum_%7Bk%3D0%7D%5E%7BK-1%7D+A_%7Bi%2Ck%7D+%5Ctimes+B_%7Bk%2Cj%7D)

（注意：这里索引从0开始，符合编程习惯）

但就是这么简单的运算，计算量大得吓人。1024×1024的矩阵乘法，需要2×1024³ ≈ 21.5亿次浮点运算。要是没有硬件加速，CPU能算到你怀疑人生。我记得第一次在树莓派上跑矩阵乘法，等了快5分钟才出结果，当时就想砸了它。

## 内存带宽：永远的痛

理论上矩阵乘法很简单，但实际跑起来问题一大堆。最要命的就是内存带宽。这个坑，每个做HPC的人都踩过。

举个例子，1024×1024的FP32矩阵乘法：

- 输入数据：2个1024×1024×4字节 = 8MB
- 输出数据：1024×1024×4字节 = 4MB
- 总共12MB数据

看起来不多对吧？

错了！大错特错！

实际运算中，每个数据可能要被访问多次。如果用最朴素的三重循环：

B矩阵是按列访问的，每次都会cache miss。实际内存访问量可能是理论值的好几倍！我第一次测试的时候，内存带宽占用率达到了98%，CPU使用率却只有15%，简直是灾难。

后来用分块优化，效果立竿见影：

测试结果（自研RISC-V芯片，2MB L2 Cache）：

- 16×16块：18.7 GFLOPS，L1命中率98.2%
- 32×32块：22.4 GFLOPS，L1命中率94.6%（最优）
- 64×64块：19.8 GFLOPS，L1命中率78.4%
- 不分块：3.2 GFLOPS（慢得离谱）

**32×32是最优的**，再大就会频繁换出cache，性能反而下降。这种细节不自己试真的不知道。当时为了找这个最优值，跑了一晚上的benchmark，眼睛都熬红了。

## 脉动阵列：专门为矩阵运算而生的架构

## Google TPU的秘密武器

第一次听说脉动阵列（Systolic Array）是看Google的TPU论文。

当时就想，为啥不用GPU那种并行架构？

后来才明白，脉动阵列简直是为矩阵运算量身定制的。

它的核心思想很简单但很妙：**让数据像血液一样有节奏地"脉动"通过计算单元**。

![图片描述](https://picx.zhimg.com/v2-9796c573d6eaff0fde61ebef168e714a_r.jpg?source=2c26e567)

传统架构每次运算都要从内存读数据，算完再写回去，内存带宽很快就成为瓶颈。

脉动阵列不一样，数据进入阵列后，会在PE（处理单元）之间流动，每个数据都被充分利用。

## 自己撸一个脉动阵列

去年没啥项目时闲得蛋疼，决定在FPGA上实现一个16×16的脉动阵列。

本以为很简单，结果被各种细节坑惨了。

先说最基本的PE设计（Verilog实现）：

在Xilinx VCU128上综合的结果：

- 200MHz时钟（再高就timing不过了，气死）
- 资源占用：约20%的LUT，30%的DSP
- 峰值性能：16×16×2×200M = 102.4 GOPS（INT8）
- 实测效率：96.9%（接近理论值！）

时序！

对于16×16的脉动阵列，第一个有效输出要等2×16-1=31个周期。

这个延迟必须精确计算，否则数据全是错的。

为这个bug，我调了整整两天，头发都快掉光了。

说句实话，一开始我还以为是32个周期，后来仔细看了公式才发现是31个。

## Weight Stationary vs Output Stationary

脉动阵列有三种数据流模式，每种都有优缺点。

我都试过，最后选了Weight Stationary（WS）。

为啥？

因为省电！当时电源模块发热严重，必须降功耗。

测试数据（28nm工艺综合，用Synopsys DC）：

- Weight Stationary：3.35mW（权重不动，激活和部分和流动）
- Output Stationary：4.28mW（部分和不动，其他流动）
- Input Stationary：4.66mW（输入不动，其他流动）

WS模式下，权重加载一次可以处理很多批数据，DRAM访问大幅减少。

对于边缘设备，这点功耗差异真的很关键。

有一次演示的时候，竞品的方案因为发热太严重，跑了半小时就降频了，我们的稳如老狗。

## Flash Attention：让Transformer飞起来

## 内存爆炸的噩梦

去年给客户部署BERT模型，差点被内存问题搞崩溃。

客户的需求很简单：“在我们的边缘盒子上跑BERT”。

我当时想，BERT而已，能有多难？

结果一上手就傻眼了。

标准Attention的内存需求是**O(N²)**，N是序列长度。

算算看：

- 序列长度512：512²×4字节 = 1MB（还行）
- 序列长度2048：2048²×4字节 = 16MB（有点大）
- 序列长度8192：8192²×4字节 = 256MB（爆炸！）

这还只是单层单头！

BERT-Large有24层，每层16个头。序列长度2048时，所有层和所有头的Attention矩阵总和需要：24×16×16MB = 6GB内存。**边缘设备哪有这么多内存？**客户的盒子总共才8GB。

## Flash Attention的天才想法

Flash Attention的核心思想其实很简单：**不存储完整的Attention矩阵**。

但实现起来，细节多得很。

关键是增量Softmax，这个算法很巧妙（我看了三遍论文才完全理解）：

我在平头哥C910上实现了完整版本，测试结果让人震惊：

**序列越长，Flash Attention优势越大**。

2048长度时快了近4倍，内存只用了5%！当时给客户演示的时候，他们都不敢相信。

## RISC-V上的优化技巧

在RISC-V上实现Flash Attention，有几个坑要特别注意：

- 向量指令的使用：RVV可以大幅加速矩阵乘法，但要注意向量寄存器的数量有限（32个）。我一开始太贪心，想一次处理太多数据，结果寄存器溢出，性能反而下降。
- 块大小的选择：不是越大越好！要考虑L1 Cache大小。我测试发现64×64是最优的，再大就会频繁cache miss。这个值是试出来的，跑了一晚上的grid search。
- 数值稳定性：增量softmax容易数值溢出，必须用LogSumExp技巧。被这个坑了好几次，输出全是NaN，查了半天才发现是数值溢出。

## 量化：用INT8跑AI模型的那些坑

## 为什么要量化？说白了就是穷

去年项目预算被砍了一半，老板说："能不能用便宜点的芯片？"我说：“那就只能量化了。”

FP32精度很好，但太费资源了。

一个ResNet-50模型，FP32要97.5MB，INT8只要24.4MB，省了75%！更要命的是，客户的破芯片只有128MB内存，不量化根本跑不动。

更重要的是速度。

我在自研芯片上测试：

- FP32乘法：4-5个周期（还得看心情）
- INT8乘法：1个周期（稳定）

INT8版本的ResNet-50比FP32快3.5倍！

当时演示的时候，客户眼睛都亮了。

## 量化的数学原理：没那么玄乎

量化就是把浮点数映射到整数。说得高大上，其实就是缩放+取整。

量化公式（，明确有符号INT8）：

- 量化：
- 反量化：

其中S是缩放因子（scale），Z是零点（zero point）。

这里有个大坑要说清楚：**INT8到底是有符号还是无符号？**

- 有符号INT8：-128到127（对称量化常用）
- 无符号UINT8：0到255（非对称量化常用）

我被这个坑过好几次。

有次把有符号当无符号用，结果负数全变成了巨大的正数，模型输出一堆NaN，调了整整一天才发现。

## 动态量化 vs 静态量化：各有各的坑

静态量化需要校准数据集，找最优的scale和zero_point。动态量化实时计算，灵活但慢。

我的经验：**CNN用静态，Transformer用动态**。

为啥？

CNN的激活分布比较稳定，静态量化够用。

Transformer的注意力分数变化很大，必须动态调整。

## 量化感知训练（QAT）：训练时就开始装傻

直接量化（PTQ）会损失精度，特别是小模型。

QAT在训练时就模拟量化，让模型适应量化误差。

核心技巧是**伪量化（Fake Quantization）**：前向传播时量化，反向传播时用直通估计器（STE）。

QAT的坑很多，最大的是**梯度消失**。

量化相当于给梯度加了个阈值函数，很容易把梯度截断。

解决办法是用**可学习的量化参数**：

用QAT训练的模型，精度损失可以控制在1%以内。实测结果：

**QAT确实比PTQ好，但训练时间要多3倍**。如果时间紧，PTQ也够用。搞了半天，精度差距其实不大，主要看应用场景。

## 大模型推理：从爆内存到丝滑运行

## KV Cache：大模型的内存杀手

去年部署LLaMA-7B，被KV Cache搞得快疯了。

什么是KV Cache？我给你算笔账就知道了。

生成式模型是自回归的，每生成一个token都要看之前所有token。如果每次都重算：

- 生成第1个token：计算1次attention
- 生成第2个token：计算2次attention
- 生成第n个token：计算n次attention
- 总计算量：O(n²)，慢得离谱！

所以要缓存之前的K和V矩阵。但这个Cache太大了：

- 单个序列：seq_len × hidden_size × 2（K和V）× num_layers × sizeof(fp16)
- LLaMA-7B（2048长度）：2048 × 4096 × 2 × 32 × 2字节 = 1GB！
- 要支持batch=16？16GB内存没了。

当时客户说："我们的盒子只有8GB内存。"

我差点当场辞职。

## PagedAttention：借鉴操作系统的天才设计

vLLM的PagedAttention简直是救命稻草。核心思想：**像管理虚拟内存一样管理KV Cache**。

传统做法是预分配最大长度的Cache，利用率极低。大部分序列只有几百个token，但要预留2048的空间，**内存利用率不到20%**。

PagedAttention的做法：

- 把KV Cache分成固定大小的块（Page）
- 按需分配，用完再申请
- 支持Copy-on-Write（多个序列共享前缀）

实测效果（LLaMA-7B，批大小16）：

- 传统方式：需要16GB内存，利用率约20%
- PagedAttention：只需3.2GB，利用率95%+
- 内存节省80%！

但PagedAttention也有坑，最大的是**内存碎片**。用久了会有很多小块无法合并，需要定期整理：

## Continuous Batching：榨干每一个计算周期

传统批处理要等最慢的序列，快的完成了也得等着，GPU在摸鱼。

Continuous Batching不等，完成的立即退出，新的立即加入。

但实现起来坑很多，最大的问题是**调度公平性**。

如果不控制，短请求会饿死长请求。

测试结果，吞吐量提升**2.4倍**！

P95延迟降低**35%**。

当时看到这个数据，我激动得差点跳起来。

## RISC-V与专用硬件的协同

## 为什么要软硬协同？

去年做了个实验，纯软件优化 vs 软硬协同，结果很有意思：

**软件优化有上限，硬件加速是必经之路**。

## 自定义指令：为AI量身定制

RISC-V最大的优势是可以添加自定义指令。

我们添加了几条专门的AI指令：

但要注意，自定义指令不能乱加。每条指令都要：

- 频繁使用（利用率>5%）
- 软件难以高效实现
- 硬件实现简单
- 不破坏流水线

我们加了8条自定义指令，芯片面积只增加了3%，但AI性能提升了8倍！这投入产出比，简直不要太爽。

## NPU集成：不是简单的加法

把NPU集成到RISC-V SoC不是简单地把两个IP核放一起。最大的挑战是**数据搬运**。

软硬件协同的关键是**隐藏数据传输延迟**。用双缓冲：

这样优化后，**数据传输和计算完全重叠**，性能提升40%！

当时测试出这个结果，整个团队都嗨翻了。

差不多了，已经写不下去了。

## 只能数数那些年踩过的大坑

## 坑1：编译器版本不兼容

去年最惨的一次，代码在GCC 12上跑得好好的，换到GCC 13直接崩溃。查了三天，发现其实不是calling convention变了，而是编译器对某些优化的处理方式改了，导致向量指令的生成有差异。

**教训**：锁定工具链版本！

## 坑2：内存对齐导致的神秘崩溃

RISC-V对未对齐访问的支持很差，性能会下降10倍，甚至直接触发异常。

## 坑3：缓存一致性的噩梦

多核RISC-V的缓存一致性很弱，不小心就会读到脏数据。

## 坑4：浮点异常处理

RISC-V的浮点异常默认是关闭的，NaN和Inf会静默传播，调试极其困难。

回头看这些年，从懵懂入门到现在能独立搞定整个AI推理栈，确实成长了很多。

技术上的收获就不说了，更重要的是心态的变化。

以前遇到bug会很烦躁，现在反而有点享受解决问题的过程。

特别是那种调了一周终于找到根因的时刻，真的很爽。

给想入坑RISC-V AI的朋友几点建议：

- 从小项目开始：别一上来就搞7B大模型，先把向量点积优化好
- 多看代码少看论文：论文的算法往往理想化，实际实现全是坑
- 建立自己的工具库：调试工具、性能分析、自动测试，这些会救你的命
- 参与开源社区：很多坑别人已经踩过了，GitHub的issue是宝藏
- 保持耐心：RISC-V生态还在发展，工具链不完善是常态

没有开源社区，就没有今天的RISC-V。

没有前辈们的分享，我可能还在某个bug里出不来。

希望这篇文章能帮到你。

如果有问题，欢迎交流。评论区见吧，不过要闲了才能回！

**P.S.**这篇文章断断续续记的笔记，很多代码是从项目里简化出来的。如有错误，欢迎指正。

**P.P.S**下一篇准备写写Chiplet和异构计算，不要期待，因为不知道猴年马月～

对了，我开圈了，有什么难题可以到哪提问，不管多难都可以：[全能解决难题异圈](https://www.zhihu.com/ring/host/1939253695989683356?share_code=lpq08DeE7BLb&utm_psn=1944094761091830670)
