---
type: resource
source: zhihu
url: https://zhuanlan.zhihu.com/p/622361287
topics: [AI, LLM, 提示工程, 模型训练, 自学, 阅读]
status: processed
para: resource
tags: []
created: 2026-02-02
updated: 2026-02-11
---
# 用知乎数据做RLHF
[内容链接](https://zhuanlan.zhihu.com/p/622361287)

RLHF是ChatGPT，Claude等chatbot的核心环节之一。

今天我们用知乎的数据尝试实现一下RLHF的整个流程。

## RLHF三步走

在语言模型预训练之后，RLHF大致分为三个阶段：

- Instruct Tuning
- Reward Model Tuning
- PPO

首先，实现RL需要一个Actor Model（生成模型）和一个Reward Model（打分模型）。

生成模型需要Instruct Tuning来学习如何follow指令，而Reward model将学习人类的preference，对Actor模型的生成结果进行打分。

完成以上两个阶段后，我们将使用Proximal Policy Optimization（PPO）来实现强化学习的整个过程。

代码部分我们使用DeepSpeed-Chat来做实现。

[https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat​github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat](https://link.zhihu.com/?target=https%3A//github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)

## Instruct Tuning

个人认为知乎的交互形式不太适合用来实现指令微调。

但作为示范的demo，我们可以直接使用知乎的问题作为Instruction，回答作为Responses。

如下表所示：




我在HuggingFace Hub上upload了包含26k数据的instruct tuning的版本，大家可以用作参考。

[liyucheng/zhihu_26k · Datasets at Hugging Face](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/liyucheng/zhihu_26k)

我们将数据套入模版中作为模型的输入：

接下来就是启动训练

这里我们使用opt-1.3b，并将batch_size设为8。我是在4*A6000上做的实验，具体超参数需要根据自己的设备进行调整。

## Reward Model Tuning

Reward model将要学习人类的preference以便对我们的生成模型进行打分。

这里我们使用知乎的点赞数来作为评判标准，将同一问题下的回答构成正负样本对（chosen-reject pair）。

在构建样本对时需要谨慎小心：

- 首先是注意重复问题，尽可能保证同一个response不出现第二次，以防止过拟合。
- 其次是，知乎的点赞受许多因素影响（例如答题人粉丝数，答题时间等），所以为了保证样本对的鲁棒性，我们应确保chosen的质量远好于reject。

这里，我们将标准设置为：创建时间相近的回答，点赞数相差至少两倍（chosen>>reject）。

如上述例子1，chosen的点赞数为1291，reject为91，chosen的点赞远大于reject，以确保构建样本对的质量。

同样，我在HuggingFace Hub上上传了这部分数据供参考：

[liyucheng/zhihu_rlhf_3k · Datasets at Hugging Face](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/liyucheng/zhihu_rlhf_3k)

Reward model将分别预测对chosen和reject的reward，并使用pairwise loss鼓励reward model对chosen给出更高的reward。

超参数的设置：

一般来说Reward model不需要和主模型具有一样的参数量。往往会采用更小的模型。

## PPO

在RL阶段，我们首先需要理解RL中的environment和action在language model意味着什么。

Environment可以简单理解为输入的prompt，action可以理解为Actor model（生成模型）所生成的文本。

所以，根据RL的pipeline，我们首先需要动态构建(environment, action)的Mini dataset。

然后，从Mini dataset中采样(environment, action) pair，交给reward model给出reward。

这里，我们需要引入Actor Model 2 （Frozen model）作为主模型对比。

Frozen model是Instruct Tuning之后的Actor model，但是不在RL训练过程中更新参数（也就是RL前的原始Actor model）。

在RL过程中，Frozen model将被用来与Actor model进行对比，用来判断更新后的Actor model对Frozen model是否有优势（advantages）。

在根据advantages计算policy gradient：

一些超参数：

## 结果

本实验使用liyucheng/zhihu_26kInstruct Tuning 3个epoches。

使用liyucheng/zhihu_rlhf_3k训练reward model一个epoch。

PPO训练一个epoch。

设备是4*A6000（24GB）

分别用时：

共用时1小时2分钟，DeepSpeed确实有飞一般的感觉。。

## 使用

Actor Model:

After RL:

可以看到RL结果十分垃圾，可以说本次使用知乎数据进行RLHF以失败而告终 ：）

首先是数据源过于单一，数据量太少；二是reward model没有精调，而且chosen-reject数据对没有清洗等等原因。

## 参考

原始数据：

[wangrui6/Zhihu-KOL · Datasets at Hugging Face](https://link.zhihu.com/?target=https%3A//huggingface.co/datasets/wangrui6/Zhihu-KOL)

DeepSpeed-Chat:

[https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat](https://link.zhihu.com/?target=https%3A//github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)
